%
% File acl07.tex
%
% Contact: sujian@i2r.a-star.edu.sg

\documentclass{article}
\usepackage{acl07}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{covington}
\usepackage{qtree}
\usepackage{pstricks,pstcol,pst-node}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Capturing Quantities in Text}
\author{Gina Cook\\
  Engineering and Computer Science\\
  Concordia University\\
  Montr\'{e}al, QC Canada\\
  {\tt gina.c.cook@gmail.ca}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}

\tableofcontents
%\listoffigures

\section{Introduction}

This paper discusses a natural language processing pipeline which uses a cyclic derivational approach to annotate quantity in a textual entailment task \S~\ref{quantityannotator}. To test the implementation of the quantity annotations a simple inferencer \S~\ref{inferencer} was built which used only the annotated quantities to determine if a hypothesis was entailed by the text. The text/hypothesis pairs were taken from gold standard data used in the RTE4 and RTE5 challenges \S~\ref{rte}. The inferencer achieved a 62.9\% F score on the pairs where quantity was needed to determine the textual entailment. 

This paper largely serves to document some of the considerations needed when working with Noun Phrases and locating quantities and head nouns inside of them. In addition, many examples which are relevant to quantity annotating are provided. 

\section{Textual Entailment}

\subsection{World Knowledge and  Old Information}
A great deal of context is not stated explicitly in the texts that we read. Texts are usually considered a tool  to add to our existing knowledge, or to connect previously existing knowledge. Newspaper texts could be considered as an excellent starting genre as the texts will generally fall neatly into background knowledge contexts (criminal acts and sentencing, celebrity gossip, natural disasters, global politics)  and often seem to provide most of the relevant information necessary to draw infernces and build knowledge. However, context genres which depart toward humour (witty magazine articles) or  opinions (blogs) often leave the obvious required background knowledge unstated for comical effect or to draw the interest of their readers. 

To deal with the additional background knowledge needed when analyzing a text, such as the fact that France is a country which is located in Europe, Iftene 2008 developed a module in his Text Entailment system which took Named Entities from the text and searched Wikipedia to construct background knowledge about those named entities to be used in the task. Of course, the hypotheses offered in Textual Entailment tasks are expected to be consistent with reality, and thus consistent with Wikipedia. However, over use of background knowledge can claim that true Hypotheses such as Bangladesh is east of India supposedly follow from a text that discusses the exports of Bangladesh and discusses nothing of its geographical position. Thus, misclassifying the hypothesis as entailed by the text, incorrectly allows the text to be offered in the justification of a question answering system which provides the users the information which it used to arrive at its answer. 

\subsection{Expectations and ``Normality"}

In addition to background knowledge, a sense of what is ``normal",  "relative" or what follows expectedly from a previous event is used by humans when building inferences from text. For example, the hypothesis that \textit{11 troops were deployed} in Example~\ref{normality} below violates the sense of normality and would cause a human to double-read the text to be sure this was indeed the fact that the text wanted to communicate.

\begin{example}{\bf Eleven troops is an unexpectedly/relatively small number of troops, and therefore unlikely}\\
\label{normality}
Britain deployed troops to Afghanistan shortly after the attacks of
		11 September, 2001. Few then thought that British forces would still
		be in Afghanistan in far larger numbers seven years on, nor that they
		would be involved in some of the fiercest fighting British forces have
		seen in decades, as part of Nato's International Security and
		Assistance Force (ISAF).
	
Britain has {\bf 11 troops} that take part in Nato's International
		Security and Assistance Force.
\end{example}


In addition to relative size, knowing that one event (mariage) is a precondition to other events (divorce) might also be useful in textual entailment. VerbOcean 2004 is a resource which identifies relations between verbs such as \textit{enablement} and \textit{happens-before} as shown in Table~\ref{verbocean} below.

\begin{example}
\label{verbocean}
VerbOcean provides relations between verbs such as Enablement and Happens-Before\\
\includegraphics[width=250px]{verbocean.pdf}
VerbOcean 2004, cited in Iftene 2008:46.
\end{example} 


\subsection{New Information and Discourse Representation}

Generally considered the main source of information, the text itself presents new information which can be represented in the Discourse Representation framework of linguists Kamp and Reyle 1996.  

\subsubsection{Entities and Named Entities} 

Noun phrases differ in their specificity and their definiteness. The specificity and definiteness of noun phrases can usually be deduced from the articles/determiners, quantifiers or plurality of the nouns. In order to add information about entities and named entities in the discourse representation of a text it is important to resolve pronouns according to gender as shown in Example~\ref{gendercoreference} below. 

\begin{example}Co-reference resolution with gendered discourse referents:\\
\label{gendercoreference}
{\bf She} was finally found at 10.35am the next day by PC David Lloyd
		George. Something had caught his eye in the undergrowth - {\bf her} blue
		school coat or the deep red of the gloves that {\bf she} had been wearing -
		and he stepped off the forestry track. {\bf  Muriel Drinkwater} was lying on
		{\bf her} back, one arm outstretched by {\bf her} side, the other slightly raised
		but with the gloved hand limp. {\bf Her} eyes were open, but there was no
		doubt that {\bf she} was dead. {\bf She} was just a few weeks short of {\bf her} 13th
		birthday. The constable blew sharply on his whistle. The year was
		1946, when murder was supposedly less commonplace than now. The
		killing of {\bf Muriel Drinkwater} was to make headlines for months and
		would vex Detective Chief Inspector William "Bulldog" Chapman, the
		Scotland Yard detective who led the inquiry, until his death nine
		years later.
		
	{\bf Muriel Drinkwater} was killed in 1946.
\end{example}

Creating co-reference chains is more complicated when there are multiple possible references, and worse still (even for humans) when the references are very similar as the two skyscrapers discussed in Example~\ref{skyscrapers} below. 

\begin{example}Co-reference chain with highly similar discourse referents:\\
\label{skyscrapers}
Seoul City said Monday {\bf a 690-meter-tall, 133-story multifunctional
		skyscraper} will be constructed in Sangam-dong. Once built, it will be
		{\bf the second highest} after  \textit{the 800-meter-high Burj Dubai}, which is under
		construction, by South Korean developer Samsung C\&T.  {\bf The
		construction} will cost more than 3.3 trillion won (\$2.37 billion), the
		city estimates. To raise funds, 23 local developers signed an MOU at a
		Seoul hotel Monday with Seoul Mayor Oh Se-hoon attending. "{\bf The
		landmark building} will help make Seoul more attractive and become a
		new tourist attraction here," Oh said.  {\bf The multifunctional building}
		will have hotels, offices, department stores, convention centers and
		various recreational facilities including an aquarium and movie
		theaters.
		
	\textit{The highest skyscraper} in the world is being built in Dubai.
\end{example}

%\subsubsection{Event Sequencing}
%\subsubsection{Relations between Events}
%\subsection{Inferencing}
\subsubsection{Closure and Disjoint Sets, exhaustiveness}

Disjoint sets in graph based logical deduction systems such as semantic web can cause very long branching before the system arrives at a conclusion of whether a statement is true or false given a certain knowledge. In addition, in order to conclude that x is a y, when give evidence that x is not a z or a w, you must know that z w and y constitute the entire domain and that the universe of discourse is closed with respect to that predicate. This is also a complexity which can increase the calculation time of entailment. Below is a more straightforward example where much of the information is present in the syntactic structure of the sentences. Knowing that only the parents who can afford to pay is a contradiction with all parents must pay is sometimes tricky to calculate if it is not explicitly stated. 


\begin{example}
Los Angeles County probation officials say they are now studying how
		other counties recover juvenile detention costs, after admitting they
		mistakenly billed parents for days when youths were held in probation
		camps and halls. By law, California counties can bill parents and
		legal guardians for some daily costs of detaining youths, but {\bf only
		those whose parents can afford to pay}. Last year, more than 20,000
		youths were admitted to probation camps and halls, and L.A. County
		billed parents a daily charge of \$11.94 for camps, \$23.63 for halls.
	
	In Los Angeles County {\bf all parents} have to pay the detention costs of
		their children.
\end{example}

\subsection{Certainty and Probability}

We as humans are rarely certain of what we conclude, thus when doing automatic inferences from text certainty must be dealt with. Iftene 2008 determines that infinitives which follow verbs/predicates such as \textit{glad, believe, claim, necessary, compulsory, free, attempt, trial, refuse, deny, ignore, plan, intend, propose, be able} are often less certain than verbs/predicates in other contexts. 

This observation by Iftene mirrors the \textit{epistemic} syntactic feature in linguistics. It is generally accepted that embedded verbs such as infinitives do have the same truth conditional semantics as main verbs. The semantics of modals, epistemic verbs and other verbs which take embedded verbs is often captured in the framework of Intensional Semantics (von Fintel and Hiem 2002).


Ultimately the degree of certainty of a verb/predicate depends on syntactic factors such as whether it is embedded, whether it is negated, whether it is qualified with an adverb, and of course whether it is typically used by speakers in an uncertain or certain fashion in that particular context (as can be seen in Example~\ref{verbs} below). It is difficult to capture the certainty of a verb simply by comparing the verb/predicates in the text and hypothesis without doing a deeper analysis and dealing with the logical form underneath the text. Fortunately in many genre's which favour objectivity, clear factual active sentences are more frequent then sentences which express uncertainty or hint at uncertainty. 


\begin{example}Sentences are rarely simple facts\\
\label{verbs}
\begin{description}
\item [Simple verbs] speed, speak, spend,  reach, raise, held, fled, lost, knew, paid \textit{Tata {\bf paid} investors 608 pence a share}
\item [Mental verbs] tried, believed, expected, thought, planned, intended, proposed, enjoyed, glad \textit{Sepulcher, traditionally {\bf believed to mark} the site where Jesus was crucified, buried and then resurrected.}
\item [Raising verbs] seemed, looked like \textit{Those who want to save Acre's forest {\bf seem to be winning}.}
\item [Control verbs] 	promised, tried \textit{But an O2 insider said there had been problems with a companies {\bf trying to sell} the pass codes.}
\item [Acc+Inf or ECM verbs] asked, wanted, enabled, expected, encouraged, pushed, forced, made, let, allowed \textit{The government now {\bf wants to use} the 450 MHz band to create a new digital wireless network that {\bf would cover} the entire country.}
\item [Modal verbs] might, can/could, will/would, ought to should \textit{The voluntary recall is considered a Class II recall since it covers products that {\bf might cause} a temporary health problem or {\bf pose} only a slight threat of a serious nature, the FDA said.}
\item [Epistemic verbs] must, has to \textit{In Los Angeles County all parents {\bf have to pay} the detention costs of their children.}
\item [Counterfactuals] would, might, attempted, denied \textit{A teenager slashed a woman to death as he {\bf attempted to steal} \$6 off her--to buy beer.}
\item [Evidential verbs] heard, saw, said, mentioned, reported \textit{The nephew of Elio Amato, an Italian mafia boss, is {\bf reported to have undergone training} at a paramilitary camp in Bulgaria.}
\item [Aspectual verbs] started, finished, kept on, continued to \textit{Kaplinsky first {\bf started to work} for the BBC in 2002 after a two year tenure with Sky News.} 
\textit{The US government {\bf wants to keep} drug prices {\bf down}.}
\end{description}
\end{example}

\section{Recognizing Textual Entailment Challenge}
\label{rte}
The RTE challenge began in 2005 and was in its fifth year in 2009. Sammons et al 2010 identify RTE and other similar `grand' challenges as the key to identifying the components needed in more complex tasks which are ultimately more useful than simple search engines and improved word counting and indexing algorithms. 

The RTE challenge has evolved to use longer texts which require co-reference resolution, identification of contradiction, implicit and explicit negation and more advanced inferencing techniques which mirror closer the real world needs in inferencing. Four tasks which serve to benefit from inferencing services are listed in \S~\ref{uses} below. The RTE challenge originally began with rather shallow processing resources which relied on bag of word models and became gradualy more complex as exemplified by the paraphrasing system in Iftene 2008 in \S~\ref{ifteneparaphrasing} and the discourse system in Hickl and Bensely in \S~\ref{hicklbensely}. 

\subsection{Uses of Textual Entailment}

\label{uses}
As one of the most linguistically challenging tasks in natural language processing recognizing textual entailments is part of tasks such as question answering and information retrieval where information must be extracted and summarized. 

\subsubsection{Question Answering}
\label{qa}
Question answering is the task of retrieving information from a single or a variety of sources to satisfy a user's question. Question answering show in Example~\ref{qaexample} below can be as simple as finding an appropriate sentence which answers the question, or as complex as compiling the information from multiple sentences in multiple documents. Question Answering essentially is one service which can be provided if information retrieval \S~\ref{retrieval}, information extraction \S~\ref{extraction}, and summarization \S~\ref{summarization} can be provided. 
\begin{example}How many employees does Kaiser have?\\
\label{qaexample}
\hspace{-.4in}\includegraphics[width=270px]{qa_example.pdf}
\end{example}

\subsubsection{Information Retrieval}
\label{retrieval}
 Information Retrieval is a task where relevant information is sorted and retrieved for the user. The text in Example~\ref{irexample} might be retrieved for a query string of \textit{detain days terror suspect}.
  
 \begin{example}Retrieve texts relevant to user queries\\
\label{irexample}
\hspace{-.4in}\includegraphics[width=270px]{ir_example.pdf}
\end{example}

\subsubsection{Information Extraction}
\label{extraction}
Information Extraction is a task which can be used to process unstructured text to build a knowledge base. In Example~\ref{ieexample} below the fact \textit{Kingston is 90 miles north of New York City} is extracted from a more general text which discusses a shopping mall in Kingston, and happens to mention the city's location with respect to New York City.

\begin{example}Extract facts/data from unstructured text\\
\label{ieexample}
\hspace{-.4in}\includegraphics[width=270px]{ie_example.pdf}
\end{example}

\subsubsection{Summarization}
\label{summarization}

Summarization is a task which can take as input one document or multiple document and produce a summary of the documents contents. At its simplest summarization can be extracting key sentences from teh document. Summarization is often considered more useful if it uses natural langauge generation and actually summarizes the information in the document(s) as shown in Example ~\ref{sumexample} below. 

\begin{example}Summarization can provide a more concise and objective statement.\\
\label{sumexample}
\hspace{-.4in}\includegraphics[width=270px]{sum_example.pdf}
\end{example}


\subsection{Paraphrasing Resources - Iftene 2008}
\label{ifteneparaphrasing}
 Iftene used a variety of resources to develop paraphrasing functionality for his textual entailment system. In a Grid computing paradigm he used a 
 \begin{itemize}
\item Lemmatizing service which found the lemmas of the words,
\item Synonym service which used WordNet, eXtended WordNet to find a set of synonyms for the verbs
\item Antonym service which used WordNet and verb ocean to find a set of antonyms to allow for passive-like transformations over the verbs
\item Dirt service which used Dirt to find a list of similar verbs for paraphrasing
\item Acronym service which finds potential full forms for acronyms
\item Background Knowledge service which starts with a Named Entity and searches Wikipedia for sentences that contain the Named Entity to build a network of Ideas about that entity.  
\end{itemize}

Iftene found that the WordNet resource, negation rules, Named Entity rules, and Contradiction rules accounted for most of his system's success as 2nd position  in the two-way task (72.1\% accuracy) and first position (68.5\% accuracy) in the three-way task of RTE4 challenge in 2008.

\subsection{Discourse Commitments - Hickl \& Bensley 2007}
\label{hicklbensely}

Hickl and Bensley 2007 use  a framework which incorporates what they call discourse commitments into the inferencing procedure. As it is a proprietary system they do not elaborate in detail on the mechanics of their system or how they extract the discourse commitments. As their results are promising (around 80\%) they indicate that incorporating discourse structure may be a profitable approach to textual entailment. 
	
%\section{Approaches to Textual Inferencing}

%\subsection{Word Overlap}

%\subsection{Logic and Discourse}

\section{Quantity Annotator}
\label{quantityannotator}

This section and the sections which follow focus on a quantity submodule which may be used to determine textual entailment. 

In order to reason over quantities a system must know both the quantity and the entitity which is quantified. Thus,  two preprocessing pipelines, number normalization \S~\ref{numbernormalization} and head noun identification \S~\ref{headnoun} were built. After these two elements have been identified quantities both exact \S~\ref{exact} and approximate \ref{approximate} are annotated as discussed.

\subsection{Number Normalization}


Out of the 46,672 word corpus, 2141 numbers were identified. Of the numbers found about half were easy to extract digits  1256 (58.7\%) , and a little less than half were more complex and required the number normalization to extract their values, 885 (41.3\%).


In development the number normalizer was compared to the CLaC number normalizer to see if an existing normalizer could be adopted. The ClaC number normalizer found 3412 numbers, many of  which were the determiner "a" and irrelevant for method of dividing plural and singleton quantities from the text. The CLaC number normalizer relied on number words and determiners to find number, where as the system described in this paper uses a cyclic derivational approach to annotate numbers and match them with noun phrases as denoting collections or denoting singletons.

\label{numbernormalization}
\subsubsection{Convert strings to values}

Using a finite state transducer all digits (9, 34, 100) which have been identified by the ANNIE tokenizer as a number are annotated as a number and their digit sequence is stored as the value which the number represents. 

String numbers (three, fourty, twenty five, couple, tr) are mapped to their values using a list of string,value pairs. 

String fractional denominators (half, quarter, thirds) are mapped to their decimal values.


\begin{example} Examples of string to value transduction
\begin{tabular}{lll}
& String & Value\\
Digits & 90 & 90\\
Strings & tr & 1000000000\\
Fractions & half & 0.5\\
Commas &  2,870,972,200 &  2870972200\\
Decimals &  7,447.55 & 7449.55\\
Sequences & half a dozen & 6\\
& ten thousand & 10000\\
& three quarters & 0.75\\
&  9.3m & 9300000\\
\end{tabular}
\end{example}

Using a finite state transducer comma separated number sequences (10,000) which are defined as a number followed by any number of a comma and 3 digit number are unified into a single number annotation with a value (10000). 

Using a finite state transducer decimal numbers (7,447.55) which are defined a number followed by a period followed by a number are unified into a single number annotation with the value taken from the value of the two numbers on either side of the period using the period as the decimal point. 

\subsubsection{Combine number sequences}

After the digit and string numbers have been given values the numbers are then combined via addtion, for example the sequence of "half a dozen" which has values \textit{0.5} and \textit{12} are multipled \textit{0.5*12= 6.0000}, " ten thousand " is calculated as \textit{10 * 1000=10000}, "three quarters" is calcualted as \textit{3*0.35=0.75}, "9.3m" is calculated as \textit{9.3*1000000=9300000}.

\subsubsection{Combine fractions}

Finally compound fractions which are defined as two numbers separated by / or "out of" are  converted into a value, for example the string "21 3/4" is converted into \textit{21 + 0.75}. This wrongly identifies dates such as ``2001/2002" as a fraction but an additional date extractor could be used to convert 2001 and 2002 into dates rather than numbers so that they would be unseen by the fraction transducer. 

\subsection{Head Noun Identification}
\label{headnoun}

After development the head noun identification pipeline was compared with MuNPEx Noun Phrase extractor.  The head noun identification looks to be very similar, which could be tested by using the head noun information from MuNPEx to see whether there would be a change in the results. 

\subsubsection{Clustering nouns with their modifiers}

Clusters of nouns and noun modifiers ("surprising secondary school dropout rate") were defined as any number of adjectives, potentially followed by a gerund (rising food prices) , followed by any number of nouns.  The system was able to identify some interesting noun groups such as "additional six-car metro trains" show in~(\ref{nouncluster}) below. The identification of this noun cluster allowed the system to recognize train as the head noun and 20 as the quantity. Thus yielding the information needed to conclude that the hypothesis "Metrorex bought 20 trains from Bombardier" is entailed by the text "Metrorex.. signed ... contract with Bombardier for... 20 additional six-car metro trains."

\begin{example}Complex noun modifiers are detected and clustered together\\
\gll additional six-car metro trains
adjective {complex measure} {modifying noun} headnoun
\glt ~
\glend

\label{nouncluster}
\hspace{-.4in}\includegraphics[width=270px]{nounclusters.pdf}
\end{example}



\subsubsection{Packages of Nouns}

Packages/groups of nouns defined as a plural noun followed by of followed by a non-count/mass noun or plural noun were also annotated. This identified many phrases such as "Large scores of Disney fans," "hundreds of thousands of pounds" and "tens of thousands of dollars" which are essentially approximate quantities, but also packages of nouns such as  "rounds of talks"  where "talks" is the semantic head noun and arguably more important for inference than "rounds" which is the syntactic head noun. 
 
While infrequent, (3 in the corpus) a pattern built to recognize packages of nouns can be useful in text-hypothesis pairs where information such as an increase in  "sales of adult party supplies"  in the text yields a contradiction with a hypothesis about an increase in  "children party supply sales." Given the way that the RTE data is constructed along entailments which are recognizable by extensions of existing NLP algorithms and against naive bag of words approaches entailments which rest on packages of nouns are very infrequent  in the RTE data. This may or may not be representative of the larger corpus of text-hypothesis pairs which are relevant to future textual entailment tasks. 


\subsubsection{Money Identification}

In addition to other packages of nouns special transducers must be made to deal with monetary amounts as they are not written in the typcial english word order of quantity noun, but rather monetary symbol quantity. 

One possible way of dealing with this is to add a token after the quantity which contains no string in the document but contains the annotaitons to indicate that it is indeed a noun and has an appropriate string (e.g. \$10 $\rightarrow$ \$ 10 dollars) 

Transducers to identify money and percent were taken from the ANNIE NE transducers and modified to add the head noun of "money" and the number's value as the quantity. 


\subsection{Exact Quantity}
\label{exact}

Exact quantities were identified as a number followed by a noun cluster which denoted a collection rather than a singleton (as defined in the noun cluster transducer).

\subsection{Approximate Quantity}
\label{approximate}

Approximate quantities were identified as potentially an downward entailment inducer (only, no, not) a qualifier (roughly, approximately, at least, less than, some, about, around, over ), a number and a noun cluster which denoted a collection. Some sample exact and approximate quantities which the system can recognize are shown in the highlighted examples throughout the document and more specifically in the examples "over 100 thousand," "168 people, "tens of thousands," "100 died" in~(\ref{approxexample}) and "About 3,000 blank passports" and "thousands of blank British passports" in~(\ref{exactexample}). 

\begin{example}Approximate quantities in the text entail exact quantities in the hypothesis.t\\
\label{approxexample}

\hspace{-.4in}\includegraphics[width=270px]{approximatetoexact.pdf}
\end{example}

\begin{example}Exact quantities in the text entail approximate quantities in the hypothesis.t\\
\label{exactexample}
\hspace{-.4in}\includegraphics[width=270px]{exactoapproximate.pdf}
\end{example}


\section{Simple Inferencer}
\label{inferencer}
To test the usability of the quantity extracting system a naive inferencing annotator was written in Java and ran over the types of quantity annotations (ExactQuantity, ApproximateQuantities, ApproximateQuantitieinCollection, Age), if  the text and hypothesis had compatible neadnouns (matching stem, or people combined with a list of people denoting headnouns such as inmates, officers, officials etc) and had compatible quantities, the pair was deemed as entailed, if not the pair was annotated as unknown. Sample inferences for ~(\ref{approxexample}) and  (\ref{exactexample}) are shown in the table in~(\ref{entailexamples}).

\begin{example}Simple inferencer\\
\label{entailexamples}
\begin{tabular}{lllll}\hline
 "over 100" &  &"100" \\
  ($100 \leq x <200$) & $\rightarrow$ &  ($100$)\\\hline
 "about 3,000" &   & "thousands"\\
 ($2500 \leq x \leq 3500$) & $\rightarrow$  & ($2000 < x < 1000000 $ )\\\hline
   
\end{tabular}
\end{example}



\section{Results}

A script in Groovy was written to extract statistics and calculate the F score of the system. A total of 265 pairs contained a quantity in the hypothesis, and thus would need information about quantity in order to determine if the hypothesis was entailed by the text. Among the pairs, 129 were considered Entailments according to the gold standard, 26 were contradictions and 110 were unknown/no entailments. 

The simple inferencing system \S~\ref{inferencer} was used to evaluate the quantity annotator discussed in \S \ref{quantityannotator}. The inferencer did not look for contradictions, if an entailment was not found it marked the pair as unknown. The inferencer identified 115 entailments,  of which 73 were correct and 42 were incorrect. The inferencer identified 150 unknowns, of which 94 were correct and 56 were incorrect. 

The precision for the entailments was 63.5\%, the recall was 56.6\%. The recall was initially 30\% until additional rules were added for determining that "100 died" referred to "100 people." This is an example where a better discourse representation would allow the headnouns of the quantity in the text and the quantity in the hypothesis to be linked together. 

The precision for the unknowns was 62.6\%, the recall was 69.9\%. One would expect the recall for the unknowns to be higher given that this was the default if the inferencer found no connection between the text and the hypothesis. 

The overall F score was 62.9\%, given that this is rather similar to the precision and recalls of both rule outputs for entailment and unknowns it indicates that the simile inferencer is not capturing the core of how to detect entailment. A more sophisticated inferencer which takes into account what the verb is for the quantities, and also the subject for this verb, essentially an inferencer which uses some representation of the text as a set or graph of facts would probably show better how the quantity annotations can be used in a textual entailment task. 




%\bibliographystyle{acl}
%\bibliography{eacl2006}

\begin{thebibliography}{}
\bibitem{hickl}Hickl, A. and Bensley, J. 2007. A Discourse Commitment-Based Framework for Recognising Textual Entailment. In \textit{Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing}. Pages 185-190. 28-29 June, Prague, Czech Republic.

\bibitem{iftene}Iftene, A. 2008. \textit{Textual Entailment}. PhD Thesis, University of Iasi, Romania.

\bibitem{kamp}Kamp, H \& U Reyle, 1996 `A Calculus for First Order Discourse
Representation Structures,' \textit{Journal of Logic, Language and Information},
5, 297Ð348.
\bibitem{heim}von Fintel, K and I Heim. 2002. \textit{Lecture Notes on Intensional Semantics}. Ms. Massachusetts Institute of Technology. 
%\bibitem{zweig}Zweig, E. 2004. Nouns and Adjectives in Numeral NPs, in {\textit Proceedings of NELS 35}.


\bibitem{sammons} Sammons, M. 2010. "Ask not what Textual Entailment can do for you." RTE Challenge 5.

\bibitem{zweig}Zweig, E. 2009. Number-Neutral Bare plurals and the Multiplicity
Implicature. \textit{Linguistics and Philosophy} Volume 32, Number 4, 353-407.
\end{thebibliography}

\end{document}


\appendix
\section{Gate Implementation - Quantity Grammar}

\section{Gate Implementation - Quantity Inferencer}


\bibitem{brent} Brent, Michael R and Xiaopeng Tao. 2001. ``Chinese text segmentation with mbdp-1: Making the most of training corpora.'' In \textit{39th Annual Meeting of the ACL}, pages 82–89.

\bibitem{creutz} Creutz, Mathias and Krista Lagus. 2005. ``Inducing the Morphological
Lexicon of a Natural Language from Unannotated Text.'' In \textit{Proceedings
of the International and Interdisciplinary Conference on Adaptive
Knowledge Representation and Reasoning (AKRR'05)}, pages 106-113,
Espoo, June. http://www.cis.hut.fi/projects/morpho/


\bibitem(deMarken)	de Marcken, Carl. 1995. ``Acquiring a lexicon from unsegmented speech.'' In \textit{33rd Annual Meeting of the ACL}, pages 311–313.

\bibitem{goldsmith} Goldsmith, J.A. 2001. ``Unsupervised Learning of the Morphology of a Natural Language.'' \textit{Computational Linguistics}, 27:2 pp. 153-198.

\bibitem{johnson} Johnson, Mark. 2008a. ``Unsupervised word segmentation for Sesotho using adaptor grammars.'' In \textit{Tenth Meeting of ACL SIGMORPHON, pages 20–27}. ACL, Morristown, NJ.
\bibitem{johnson2008} Johnson, Mark. 2008b. ``Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure.'' In \textit{46th Annual Meeting of the ACL}, pages 398–406. ACL, Morristown, NJ.
\bibitem{kanungo} Kanungo, Tapas. 1999. ``UMDHMM: Hidden Markov Model Toolkit,'' in \textit{Extended Finite State Models of Language,} A. Kornai (editor), Cambridge University Press. http://www.kanungo.com/software/software.html. \\
	http://www.umiacs.umd.edu/~resnik/nlstat\_tutorial\_summer1998
	/Lab\_hmm.html
	
\bibitem{resnik}	Resnik, Phillip. 1999. \textit{Using a Hidden Markov Model.} http://umiacs.umd.edu/~resnik/ling773\_sp2009/assignments/hmm.html
	
%\bibitem{shone} Schone, P., & Jurafsky, D. 2000. Knowledge-free induction of morphology using latent semantic analysis. In Proceedings of CoNLL-2000 and LLL-2000, pp. 67--72 Lisbon, Portugal.


\end{thebibliography}


\end{document}
